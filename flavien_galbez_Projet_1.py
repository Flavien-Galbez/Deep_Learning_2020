# -*- coding: utf-8 -*-
"""Flavien_galbez_week3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ePAwWneXjUYh7RtxBf33cBB-joakyZdF

**Part 1**

Ex 1 : loading data
"""

import tensorflow as tf

import numpy as np
from keras.datasets import imdb

tf.random.set_seed(6498)
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

"""Ex 2 : Preparation of data"""

def vectorize_sequences ( sequences , dimension =10000) :
# Create an all - zero matrix of shape ( len ( sequences ), dimension)
  results = np.zeros (( len ( sequences ) , dimension ))
  for i , sequence in enumerate ( sequences ):
    results [i,sequence ] = 1
  return results

# Our vectorized training data
x_train = vectorize_sequences (train_data)
# Our vectorized test data
x_test = vectorize_sequences (test_data)
# Our vectorized labels
y_train = np.asarray(train_labels).astype ("float32")
y_test = np.asarray(test_labels).astype ("float32")

print(x_test)
print(x_train)
print(len(x_test))
print(len(x_train))
print(y_test)
print(y_train)
print(len(y_test))
print(len(y_train))

"""Ex 3 : Creation of original_model"""

import keras
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras import Sequential

original_model = Sequential()
original_model.add(Dense(16, input_shape=(10000,), activation='relu'))
original_model.add(Dense(16, activation='relu'))
original_model.add(Dense(1, activation='sigmoid'))

original_model.compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = ["acc"]
)

"""Ex 4 : Creation of smaller_model"""

smaller_model = Sequential()
smaller_model.add(Dense(4, input_shape=(10000,), activation = 'relu'))
smaller_model.add(Dense(4,activation='relu'))
smaller_model.add(Dense(1, activation='sigmoid'))

smaller_model.compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = ["acc"]
)

"""Ex 5 : Fit original_model"""

original_hist = original_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))

"""Ex 6 : Fit smaller_model"""

smaller_hist = smaller_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data = (x_test, y_test))

"""Ex 7 : Get val_loss"""

original_loss=original_hist.history["val_loss"]
smaller_loss=smaller_hist.history["val_loss"]

"""Ex 8 : Observation of val_loss"""

import matplotlib.pyplot as plt

plt.plot(original_hist.epoch, original_loss, label = "Original Loss", color="blue")
plt.plot(smaller_hist.epoch, smaller_loss, label = "Smaller Loss", color= "red")
plt.legend()
plt.title("Loss values")

# Original model overfit at 2 epochs
# Smaller model overfit at 7 epochs

"""Ex 9 : Creation of bigger_model"""

bigger_model = Sequential()
bigger_model.add(Dense(512, input_shape=(10000,), activation='relu'))
bigger_model.add(Dense(512, activation='relu'))
bigger_model.add(Dense(1, activation='sigmoid'))

bigger_model.compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = ["acc"]
)

"""Ex 10 : Fit bigger_model"""

bigger_hist = bigger_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))

"""Ex 11 : Get val_loss of bigger_model"""

bigger_loss=bigger_hist.history["val_loss"]

plt.plot(bigger_hist.epoch, bigger_loss, label = "Bigger loss ", color="blue")
plt.plot(original_hist.epoch, original_loss, label = "Original Loss", color= "red")
plt.legend()
plt.title("Loss values")

"""Ex 12 : Modify the original network"""

from keras import regularizers

l2_model = Sequential()
l2_model.add(Dense(16, input_shape=(10000,), activation='relu', kernel_regularizer=regularizers.l2(0.001)))
l2_model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
l2_model.add(Dense(1, activation='sigmoid'))

l2_model.compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = ["acc"]
)

"""Ex 13 : Fit l2_model"""

l2_hist = l2_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))

"""Ex 14 : Plot result"""

l2_loss=l2_hist.history["val_loss"]

plt.plot(l2_hist.epoch, l2_loss, label = "l2 loss ", color="blue")
plt.plot(original_hist.epoch, original_loss, label = "Original Loss", color= "red")
plt.legend()
plt.title("Loss values")

"""Ex 15 : Creation of modified model dpt_model"""

from keras.layers import Dropout

dpt_model = Sequential()
dpt_model.add(Dropout(0.5))
dpt_model.add(Dense(16, input_shape=(10000,), activation='relu', kernel_regularizer=regularizers.l2(0.001)))
dpt_model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
dpt_model.add(Dense(1, activation='sigmoid'))

dpt_model.compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = ["acc"]
)

"""Ex 16 : Fit dpt_model"""

dpt_hist = dpt_model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))

"""Ex 17 : Plot dpt_model"""

dpt_loss= dpt_hist.history["val_loss"]

plt.plot(dpt_hist.epoch, dpt_loss, label = "dpt loss ", color="blue")
plt.plot(l2_hist.epoch, l2_loss, label = "l2 loss ", color="yellow")
plt.plot(original_hist.epoch, original_loss, label = "Original Loss", color= "red")
plt.legend()
plt.title("Loss values")

"""**Part 2**

Ex 1 : Load House Price DataSet
"""

# Commented out IPython magic to ensure Python compatibility.
try :
# % tensorflow_version only exists in Colab .
#   %tensorflow_version 2.x
except Exception :
  pass
from keras . datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data ()

print("Train rows :", len(train_data))
print("Train columns :", len(train_data[0]))
print("Test rows :", len(test_data))
print("Test columns :", len(test_data[0]))

"""Ex 2 : Normalizing data"""

norm_train_data= (train_data-train_data.mean(axis=0))/train_data.std(axis=0)
norm_train_targets= (train_targets-train_targets.mean(axis=0))/train_targets.std(axis=0)
norm_test_data= (test_data-test_data.mean(axis=0))/test_data.std(axis=0)
norm_test_targets= (test_targets-test_targets.mean(axis=0))/test_targets.std(axis=0)

"""Ex 3 : Model definition"""

from keras import models
from keras import layers
def build_model():
  model=models.Sequential()
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(1))
  model.compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = ["mae"]
)
  return model

"""Ex 4 : Model Validation"""

import numpy as np
k=4
num_val_samples = len(train_data) //k
num_epochs=100
all_scores=[]
for i in range(k):
  print('Processing fold #',i)
  val_data=norm_train_data[i * num_val_samples : (i+1) * num_val_samples]
  val_targets= norm_train_targets[i * num_val_samples : (i+1) * num_val_samples]
  partial_train_data=np.concatenate([norm_train_data[ : i * num_val_samples],
                                     norm_train_data[(i+1) * num_val_samples:]],
                                     axis=0)
  partial_train_targets=np.concatenate([norm_train_targets[:i * num_val_samples],
                                        norm_train_targets[(i+1) * num_val_samples:]],
                                        axis=0)
  model=build_model()
  model_hist = model.fit(partial_train_data,
                         partial_train_targets,
                         validation_data=(val_data, val_targets),
                         epochs=num_epochs,
                         batch_size=1,
                         verbose=0)
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores.append(val_mse)

"""Ex 5 : Overall score"""

for i in range(len(all_scores)):
  print(i, ": ", all_scores[i])
print("Average : ", np.mean(all_scores))

"""Ex 6 : Bit longuer network"""

k=4

num_epochs=500
all_my_histories = []

for i in range(k):
  print('Processing fold #',i)
  val_data=norm_train_data[i*num_val_samples : (i+1)*num_val_samples]
  val_targets= norm_train_targets[i*num_val_samples: (i+1)*num_val_samples]
  partial_train_data=np.concatenate([norm_train_data[:i*num_val_samples], train_data[(i+1)*num_val_samples:]], axis=0)
  partial_train_targets=np.concatenate([norm_train_targets[:i*num_val_samples], train_targets[(i+1)*num_val_samples:]], axis=0)
  model=build_model()
  history = model.fit(partial_train_data,
                         partial_train_targets,
                         validation_data=(val_data, val_targets),
                         epochs=num_epochs,
                         batch_size=1,
                         verbose=0)
  mae_history=history.history['val_mean_absolute_error']
  all_my_histories.append(mae_history)

"""Ex 7 : Average of the per-epoch mae scores"""

average_my_histories = [np.mean([x[i] for x in all_my_histories]) for i in range(num_epochs)]

"""Ex 8 : Plot average of MAE score"""

import matploblib.pyplot as plt
plt.plot(range(1,len(average_my_histories)+1), average_my_histories)
plt.xlabel('Number of epochs')
plt.ylabel('Validation MAE')
plt.show